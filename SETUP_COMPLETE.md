# PROJECT SETUP COMPLETE âœ…

## What's Been Created

Your **Customer Churn Prediction & Customer Lifetime Value (CLV) Analysis** project is now fully scaffolded and ready to run!

### ğŸ“ Directory Structure

```
project2-churn-prediction/
â”œâ”€â”€ README.md                    # Complete project documentation
â”œâ”€â”€ AI_USAGE.md                  # AI assistance summary
â”œâ”€â”€ requirements.txt             # All dependencies (pinned versions)
â”œâ”€â”€ .gitignore                   # Git ignore rules
â”œâ”€â”€ app.py                       # Streamlit interactive web app (3 tabs)
â”œâ”€â”€ run_pipeline.py              # One-command pipeline runner
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # For raw IBM Telco dataset
â”‚   â””â”€â”€ processed/               # Processed train/val/test splits
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_prep.py             # Data loading, cleaning, feature engineering
â”‚   â”œâ”€â”€ clv_analysis.py          # CLV calculation and business insights
â”‚   â”œâ”€â”€ train_models.py          # Train 3 models (LR, RF, XGB)
â”‚   â”œâ”€â”€ interpretability.py      # SHAP & feature importance
â”‚   â””â”€â”€ predict.py               # Prediction utilities
â”œâ”€â”€ models/                      # Trained models & results
â”œâ”€â”€ figures/                     # Generated visualizations
â””â”€â”€ notebooks/                   # Optional: exploratory analysis
```

---

## ğŸš€ Quick Start (5 Steps)

### 1. Set Up Environment
```bash
# Create virtual environment
python -m venv venv
 source venv/Scripts/activate    # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Download Data
- Get IBM Telco Customer Churn dataset from:
  - Kaggle: https://www.kaggle.com/blastchar/telco-customer-churn
  - GitHub: https://github.com/IBM/telco-customer-churn-on-icp4d/tree/master/data
- Save to: `data/raw/WA_Fn-UseC_-_Telco_Customer_Churn.csv`

### 3. Run Complete Pipeline
```bash
python run_pipeline.py
```
This runs all 5 steps automatically:
- Data prep (clean, engineer features)
- CLV analysis (segment customers)
- Model training (3 models + evaluation)
- Interpretability (SHAP + feature importance)
- Output (models, plots, results)

### 4. Review Results
```bash
# Check model performance
cat models/test_results.csv

# Check business insights
open figures/churn_by_clv.png
```

### 5. Launch Web App
```bash
streamlit run app.py
```
Open browser to `http://localhost:8501`

---

## ğŸ“Š What Each Component Does

### Data Preparation (`src/data_prep.py`)
âœ… Loads IBM Telco dataset  
âœ… Handles missing TotalCharges (fills with `MonthlyCharges Ã— tenure`)  
âœ… Engineers 4 explainable features  
âœ… Calculates CLV using formula: `MonthlyCharges Ã— ExpectedTenure`  
âœ… Splits into train/val/test (60/20/20) with stratification  
âœ… Encodes categorical variables (LabelEncoder, alphabetical order)  

### CLV Analysis (`src/clv_analysis.py`)
âœ… Computes CLV per customer  
âœ… Segments into quartiles (Low, Medium, High, Premium)  
âœ… Analyzes churn rate by segment  
âœ… Generates business insights  
âœ… Creates visualizations (saved to `figures/`)  

### Model Training (`src/train_models.py`)
âœ… Trains 3 models:
  - Logistic Regression (baseline, interpretable)
  - Random Forest (ensemble, robust)
  - XGBoost (state-of-the-art, tuned)  
âœ… Light hyperparameter tuning  
âœ… Evaluates on test set (Precision, Recall, F1, AUC)  
âœ… Creates ROC curves and confusion matrices  
âœ… Saves models to `models/` directory  

### Interpretability (`src/interpretability.py`)
âœ… SHAP TreeExplainer for tree models (RF, XGBoost)  
âœ… Standardized coefficients for Logistic Regression  
âœ… Global feature importance (top 15 per model)  
âœ… Saves importance tables to CSV  
âœ… Creates visualization plots  

### Streamlit App (`app.py`)
âœ… **Tab 1 - Predict**:
  - Input customer features
  - View churn probability + risk label
  - See estimated CLV
  - Model agreement & feature importance  

âœ… **Tab 2 - Model Performance**:
  - Metrics table (all 3 models)
  - ROC curves overlay
  - Confusion matrices
  - Global feature importance  

âœ… **Tab 3 - CLV Overview**:
  - CLV distribution histogram
  - Churn rate by segment
  - Business takeaway  

---

## ğŸ¯ Expected Results

### Model Performance (Test Set)
- **Accuracy**: ~80%
- **Precision**: ~65%
- **Recall**: 60â€“70% â† Important! Catch most churners
- **AUC-ROC**: ~84%

### Feature Importance (Top Features)
- Tenure (strong negative, more tenure = less churn)
- Contract (month-to-month = high risk)
- Services (bundles reduce churn)
- Monthly charges (sometimes proxy for tenure)

### CLV Insights
- Low-CLV customers: 50%+ churn rate
- High/Premium: <5% churn rate
- **Implication**: Focus retention on high-value segments

---

## ğŸ”§ Customization Options

### Change CLV Expected Tenure
Edit `src/data_prep.py`, line with:
```python
def calculate_clv(df, monthly_charge_col='MonthlyCharges', expected_tenure_months=24):
    # Change 24 to your assumed tenure (e.g., 36 for 3 years)
```

### Adjust Hyperparameters
Edit `src/train_models.py` for:
- Logistic Regression: `max_iter`, `class_weight`
- Random Forest: `max_depth`, `min_samples_leaf`
- XGBoost: `max_depth`, `learning_rate`, `subsample`

### Change Risk Thresholds
Edit `src/predict.py` in `get_churn_risk_label()`:
```python
if churn_probability < 0.3:      # Change thresholds
    return 'Low Risk'
```

---

## ğŸ“š File Descriptions

| File | Purpose |
|------|---------|
| `README.md` | Full project documentation (start here) |
| `AI_USAGE.md` | What AI helped with, key decisions |
| `requirements.txt` | Python dependencies (pinned versions) |
| `app.py` | Streamlit web app (launch with `streamlit run app.py`) |
| `run_pipeline.py` | Runs all pipeline steps in sequence |
| `src/data_prep.py` | Data loading, cleaning, feature engineering |
| `src/clv_analysis.py` | CLV computation, segmentation, insights |
| `src/train_models.py` | Model training, evaluation, ROC curves |
| `src/interpretability.py` | SHAP explainers, feature importance |
| `src/predict.py` | Single-customer prediction utilities |

---

## ğŸ§ª Verification Checklist

Before declaring success, verify:

- [ ] Downloaded IBM Telco dataset to `data/raw/`
- [ ] `python run_pipeline.py` completes without errors
- [ ] `models/test_results.csv` shows AUC-ROC â‰¥ 0.80 and Recall â‰¥ 0.60
- [ ] `figures/` directory contains all plots
- [ ] `models/` contains 3 pkl files + CSV importance tables
- [ ] `streamlit run app.py` launches without errors
- [ ] App displays all 3 tabs correctly
- [ ] Predict tab accepts inputs and shows churn probability
- [ ] Model Performance tab shows ROC curves and metrics
- [ ] CLV Overview tab displays business insights

---

## ğŸš¢ Next Steps (Deployment)

### For Local Development
1. Keep iterating, tuning hyperparameters
2. Test with different customer profiles
3. Validate business insights against domain experts

### For Production/Sharing
1. Push to GitHub:
   ```bash
   git init
   git add .
   git commit -m "Initial: Churn prediction & CLV analysis"
   git push origin main
   ```

2. Deploy to Streamlit Cloud:
   - Go to https://streamlit.io/cloud
   - Click "New app" â†’ Select repo, branch, file (`app.py`)
   - Deploy (automatic)
   - Share public URL

3. Ensure requirements.txt is complete:
   ```bash
   pip freeze > requirements.txt  # (Optional: update with frozen versions)
   ```

---

## ğŸ’¡ Pro Tips

1. **Model Training**: The script trains on combined train+val data (as per spec) for final models. This maximizes training data.

2. **Feature Engineering**: All features are explainable and business-relevant (no black-box embeddings). This makes the model trustworthy.

3. **SHAP**: Tree models use TreeExplainer (fast). Logistic Regression uses standardized coefficients (faster + more interpretable than KernelExplainer).

4. **Caching**: Streamlit app caches models persistently and data for 60 minutes. This makes the app snappy.

5. **Ensemble**: Final prediction is the average of all 3 models. This reduces variance and improves robustness.

---

## ğŸ“ Support & Troubleshooting

### Common Issues

**"ModuleNotFoundError"**
â†’ Run `pip install -r requirements.txt`

**"Models not found"**
â†’ Run `python src/train_models.py` to train

**"Port 8501 in use"**
â†’ Use `streamlit run app.py --server.port=8502`

**"SHAP import error"**
â†’ Install with: `pip install shap==0.43.0`

For more, see **Troubleshooting** section in `README.md`.

---

## ğŸ“– Learning Resources

- **Streamlit**: https://docs.streamlit.io/
- **SHAP**: https://shap.readthedocs.io/
- **XGBoost**: https://xgboost.readthedocs.io/
- **Scikit-Learn**: https://scikit-learn.org/
- **Churn Prediction**: https://www.kaggle.com/blastchar/telco-customer-churn

---

## âœ¨ You're All Set!

Everything is scaffolded and ready. The next steps are:

1. **Download the data** (IBM Telco dataset)
2. **Run the pipeline** (`python run_pipeline.py`)
3. **Launch the app** (`streamlit run app.py`)
4. **Deploy when ready** (Streamlit Cloud)

Questions? Check `README.md` for detailed instructions!

**Happy analyzing! ğŸ‰**

---

**Last Updated**: January 2026
